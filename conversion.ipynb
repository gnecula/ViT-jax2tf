{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8J1g5vBT5aj"
   },
   "source": [
    "## References\n",
    "\n",
    "* https://github.com/google/jax/blob/main/jax/experimental/jax2tf/examples/README.md\n",
    "* https://github.com/google-research/vision_transformer/blob/main/vit_jax.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piv05HW04aUW"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLVMT01KScv5",
    "outputId": "554ffa81-d556-49b4-f1b3-ece66dbee74c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 179 kB 8.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 485.7 MB 14 kB/s \n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 49.9 MB/s \n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 1.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 27.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 41.0 MB/s \n",
      "\u001b[K     |████████████████████████████████| 463 kB 52.2 MB/s \n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.6.0 requires flatbuffers~=1.12.0, but you have flatbuffers 2.0 which is incompatible.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q absl-py>=0.12.0 chex>=0.0.7 clu>=0.0.3 einops>=0.3.0\n",
    "!pip install -q flax==0.3.3 ml-collections==0.1.0 tf-nightly\n",
    "!pip install -q numpy>=1.19.5 pandas>=1.1.0\n",
    "!pip install -q tensorflow-datasets>=4.0.1 tensorflow-probability>=0.11.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIYdn1woOS1n",
    "outputId": "c7185643-fe10-4259-b418-eb1fb085ccf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'vision_transformer'...\n",
      "remote: Enumerating objects: 37, done.\u001b[K\n",
      "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
      "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
      "remote: Total 37 (delta 4), reused 33 (delta 4), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (37/37), done.\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "# Clone repository and pull latest changes.\n",
    "![ -d vision_transformer ] || git clone --depth=1 https://github.com/google-research/vision_transformer\n",
    "!cd vision_transformer && git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwBrIdAE4ciM"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aWuEnEshSdzt"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"./vision_transformer\" not in sys.path:\n",
    "    sys.path.append(\"./vision_transformer\")\n",
    "\n",
    "from vit_jax import models\n",
    "from vit_jax import checkpoint\n",
    "from vit_jax.configs import common as common_config\n",
    "from vit_jax.configs import models as models_config\n",
    "\n",
    "from jax.experimental import jax2tf\n",
    "import tensorflow as tf\n",
    "import flax\n",
    "import jax\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFV5xTgW28ys",
    "outputId": "9f99758f-35ee-4a08-e1f1-1888aa1e4d3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX version: 0.2.19\n",
      "FLAX version: 0.3.3\n",
      "TensorFlow version: 2.7.0-dev20210917\n"
     ]
    }
   ],
   "source": [
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"FLAX version: {flax.__version__}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKh0k1M7SgSN",
    "outputId": "da0cf212-3405-4984-b30a-2ac361f05d63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type selected: ViT-R50_L_32\n"
     ]
    }
   ],
   "source": [
    "#@title Choose a model type\n",
    "VIT_MODELS = \"R50_L_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224\" #@param [\"L_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224\", \"B_16-i21k-300ep-lr_0.001-aug_medium2-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224\", \"R50_L_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224\", \"R26_S_32-i21k-300ep-lr_0.001-aug_light0-wd_0.03-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.03-res_224\", \"R26_S_32-i21k-300ep-lr_0.001-aug_medium2-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_224\", \"S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224\", \"B_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224\"]\n",
    "#@markdown `B_8` model type is _**not**_ present inz the model configurations. The models were selected based on the criteria shown here in [this notebook](https://github.com/sayakpaul/ViT-jax2tf/blob/main/model-selector.ipynb).\n",
    "\n",
    "print(f\"Model type selected: ViT-{VIT_MODELS.split('-')[0]}\")\n",
    "\n",
    "ROOT_GCS_PATH = \"gs://vit_models/augreg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "6ZVCHqew2LcN"
   },
   "outputs": [],
   "source": [
    "#@title Slightly modified VisionTransformer model class from the [official repository](https://github.com/google-research/vision_transformer)\n",
    "# Copyright 2021 Google LLC.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from typing import Any, Callable, Optional, Tuple\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from vit_jax import models_mixer\n",
    "from vit_jax import models_resnet\n",
    "\n",
    "Array = Any\n",
    "PRNGKey = Any\n",
    "Shape = Tuple[int]\n",
    "Dtype = Any\n",
    "\n",
    "\n",
    "class IdentityLayer(nn.Module):\n",
    "  \"\"\"Identity layer, convenient for giving a name to an array.\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    return x\n",
    "\n",
    "\n",
    "class AddPositionEmbs(nn.Module):\n",
    "  \"\"\"Adds (optionally learned) positional embeddings to the inputs.\n",
    "\n",
    "  Attributes:\n",
    "    posemb_init: positional embedding initializer.\n",
    "  \"\"\"\n",
    "\n",
    "  posemb_init: Callable[[PRNGKey, Shape, Dtype], Array]\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs):\n",
    "    \"\"\"Applies AddPositionEmbs module.\n",
    "\n",
    "    By default this layer uses a fixed sinusoidal embedding table. If a\n",
    "    learned position embedding is desired, pass an initializer to\n",
    "    posemb_init.\n",
    "\n",
    "    Args:\n",
    "      inputs: Inputs to the layer.\n",
    "\n",
    "    Returns:\n",
    "      Output tensor with shape `(bs, timesteps, in_dim)`.\n",
    "    \"\"\"\n",
    "    # inputs.shape is (batch_size, seq_len, emb_dim).\n",
    "    assert inputs.ndim == 3, ('Number of dimensions should be 3,'\n",
    "                              ' but it is: %d' % inputs.ndim)\n",
    "    pos_emb_shape = (1, inputs.shape[1], inputs.shape[2])\n",
    "    pe = self.param('pos_embedding', self.posemb_init, pos_emb_shape)\n",
    "    return inputs + pe\n",
    "\n",
    "\n",
    "class MlpBlock(nn.Module):\n",
    "  \"\"\"Transformer MLP / feed-forward block.\"\"\"\n",
    "\n",
    "  mlp_dim: int\n",
    "  dtype: Dtype = jnp.float32\n",
    "  out_dim: Optional[int] = None\n",
    "  dropout_rate: float = 0.1\n",
    "  kernel_init: Callable[[PRNGKey, Shape, Dtype],\n",
    "                        Array] = nn.initializers.xavier_uniform()\n",
    "  bias_init: Callable[[PRNGKey, Shape, Dtype],\n",
    "                      Array] = nn.initializers.normal(stddev=1e-6)\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs, *, deterministic):\n",
    "    \"\"\"Applies Transformer MlpBlock module.\"\"\"\n",
    "    actual_out_dim = inputs.shape[-1] if self.out_dim is None else self.out_dim\n",
    "    x = nn.Dense(\n",
    "        features=self.mlp_dim,\n",
    "        dtype=self.dtype,\n",
    "        kernel_init=self.kernel_init,\n",
    "        bias_init=self.bias_init)(  # pytype: disable=wrong-arg-types\n",
    "            inputs)\n",
    "    x = nn.gelu(x)\n",
    "    x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
    "    output = nn.Dense(\n",
    "        features=actual_out_dim,\n",
    "        dtype=self.dtype,\n",
    "        kernel_init=self.kernel_init,\n",
    "        bias_init=self.bias_init)(  # pytype: disable=wrong-arg-types\n",
    "            x)\n",
    "    output = nn.Dropout(\n",
    "        rate=self.dropout_rate)(\n",
    "            output, deterministic=deterministic)\n",
    "    return output\n",
    "\n",
    "\n",
    "class Encoder1DBlock(nn.Module):\n",
    "  \"\"\"Transformer encoder layer.\n",
    "\n",
    "  Attributes:\n",
    "    inputs: input data.\n",
    "    mlp_dim: dimension of the mlp on top of attention block.\n",
    "    dtype: the dtype of the computation (default: float32).\n",
    "    dropout_rate: dropout rate.\n",
    "    attention_dropout_rate: dropout for attention heads.\n",
    "    deterministic: bool, deterministic or not (to apply dropout).\n",
    "    num_heads: Number of heads in nn.MultiHeadDotProductAttention\n",
    "  \"\"\"\n",
    "\n",
    "  mlp_dim: int\n",
    "  num_heads: int\n",
    "  dtype: Dtype = jnp.float32\n",
    "  dropout_rate: float = 0.1\n",
    "  attention_dropout_rate: float = 0.1\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs, *, deterministic):\n",
    "    \"\"\"Applies Encoder1DBlock module.\n",
    "\n",
    "    Args:\n",
    "      inputs: Inputs to the layer.\n",
    "      deterministic: Dropout will not be applied when set to true.\n",
    "\n",
    "    Returns:\n",
    "      output after transformer encoder block.\n",
    "    \"\"\"\n",
    "\n",
    "    # Attention block.\n",
    "    assert inputs.ndim == 3, f'Expected (batch, seq, hidden) got {inputs.shape}'\n",
    "    x = nn.LayerNorm(dtype=self.dtype)(inputs)\n",
    "    x = nn.MultiHeadDotProductAttention(\n",
    "        dtype=self.dtype,\n",
    "        kernel_init=nn.initializers.xavier_uniform(),\n",
    "        broadcast_dropout=False,\n",
    "        deterministic=deterministic,\n",
    "        dropout_rate=self.attention_dropout_rate,\n",
    "        num_heads=self.num_heads)(\n",
    "            x, x)\n",
    "    x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)\n",
    "    x = x + inputs\n",
    "\n",
    "    # MLP block.\n",
    "    y = nn.LayerNorm(dtype=self.dtype)(x)\n",
    "    y = MlpBlock(\n",
    "        mlp_dim=self.mlp_dim, dtype=self.dtype, dropout_rate=self.dropout_rate)(\n",
    "            y, deterministic=deterministic)\n",
    "\n",
    "    return x + y\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  \"\"\"Transformer Model Encoder for sequence to sequence translation.\n",
    "\n",
    "  Attributes:\n",
    "    num_layers: number of layers\n",
    "    mlp_dim: dimension of the mlp on top of attention block\n",
    "    num_heads: Number of heads in nn.MultiHeadDotProductAttention\n",
    "    dropout_rate: dropout rate.\n",
    "    attention_dropout_rate: dropout rate in self attention.\n",
    "  \"\"\"\n",
    "\n",
    "  num_layers: int\n",
    "  mlp_dim: int\n",
    "  num_heads: int\n",
    "  dropout_rate: float = 0.1\n",
    "  attention_dropout_rate: float = 0.1\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs, *, train):\n",
    "    \"\"\"Applies Transformer model on the inputs.\n",
    "\n",
    "    Args:\n",
    "      inputs: Inputs to the layer.\n",
    "      train: Set to `True` when training.\n",
    "\n",
    "    Returns:\n",
    "      output of a transformer encoder.\n",
    "    \"\"\"\n",
    "    assert inputs.ndim == 3  # (batch, len, emb)\n",
    "\n",
    "    x = AddPositionEmbs(\n",
    "        posemb_init=nn.initializers.normal(stddev=0.02),  # from BERT.\n",
    "        name='posembed_input')(\n",
    "            inputs)\n",
    "    x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=not train)\n",
    "\n",
    "    # Input Encoder\n",
    "    for lyr in range(self.num_layers):\n",
    "      x = Encoder1DBlock(\n",
    "          mlp_dim=self.mlp_dim,\n",
    "          dropout_rate=self.dropout_rate,\n",
    "          attention_dropout_rate=self.attention_dropout_rate,\n",
    "          name=f'encoderblock_{lyr}',\n",
    "          num_heads=self.num_heads)(\n",
    "              x, deterministic=not train)\n",
    "    encoded = nn.LayerNorm(name='encoder_norm')(x)\n",
    "\n",
    "    return encoded\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "  \"\"\"VisionTransformer.\"\"\"\n",
    "\n",
    "  num_classes: int\n",
    "  patches: Any\n",
    "  transformer: Any\n",
    "  hidden_size: int\n",
    "  resnet: Optional[Any] = None\n",
    "  representation_size: Optional[int] = None\n",
    "  classifier: str = 'token'\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs, *, train):\n",
    "\n",
    "    x = inputs\n",
    "    # (Possibly partial) ResNet root.\n",
    "    if self.resnet is not None:\n",
    "      width = int(64 * self.resnet.width_factor)\n",
    "\n",
    "      # Root block.\n",
    "      x = models_resnet.StdConv(\n",
    "          features=width,\n",
    "          kernel_size=(7, 7),\n",
    "          strides=(2, 2),\n",
    "          use_bias=False,\n",
    "          name='conv_root')(\n",
    "              x)\n",
    "      x = nn.GroupNorm(name='gn_root')(x)\n",
    "      x = nn.relu(x)\n",
    "      x = nn.max_pool(x, window_shape=(3, 3), strides=(2, 2), padding='SAME')\n",
    "\n",
    "      # ResNet stages.\n",
    "      if self.resnet.num_layers:\n",
    "        x = models_resnet.ResNetStage(\n",
    "            block_size=self.resnet.num_layers[0],\n",
    "            nout=width,\n",
    "            first_stride=(1, 1),\n",
    "            name='block1')(\n",
    "                x)\n",
    "        for i, block_size in enumerate(self.resnet.num_layers[1:], 1):\n",
    "          x = models_resnet.ResNetStage(\n",
    "              block_size=block_size,\n",
    "              nout=width * 2**i,\n",
    "              first_stride=(2, 2),\n",
    "              name=f'block{i + 1}')(\n",
    "                  x)\n",
    "\n",
    "    n, h, w, c = x.shape\n",
    "\n",
    "    # We can merge s2d+emb into a single conv; it's the same.\n",
    "    x = nn.Conv(\n",
    "        features=self.hidden_size,\n",
    "        kernel_size=self.patches.size,\n",
    "        strides=self.patches.size,\n",
    "        padding='VALID',\n",
    "        name='embedding')(\n",
    "            x)\n",
    "\n",
    "    # Here, x is a grid of embeddings.\n",
    "\n",
    "    # Transformer.\n",
    "    n, h, w, c = x.shape\n",
    "    x = jnp.reshape(x, [n, h * w, c])\n",
    "\n",
    "    # If we want to add a class token, add it here.\n",
    "    if self.classifier == 'token':\n",
    "      cls = self.param('cls', nn.initializers.zeros, (1, 1, c))\n",
    "      cls = jnp.tile(cls, [n, 1, 1])\n",
    "      x = jnp.concatenate([cls, x], axis=1)\n",
    "\n",
    "    x = Encoder(name='Transformer', **self.transformer)(x, train=train)\n",
    "\n",
    "    if self.classifier == 'token':\n",
    "      x = x[:, 0]\n",
    "    elif self.classifier == 'gap':\n",
    "      x = jnp.mean(x, axis=list(range(1, x.ndim - 1)))  # (1,) or (1,2)\n",
    "    else:\n",
    "      raise ValueError(f'Invalid classifier={self.classifier}')\n",
    "\n",
    "    if self.representation_size is not None:\n",
    "      x = nn.Dense(features=self.representation_size, name='pre_logits')(x)\n",
    "      x = nn.tanh(x)\n",
    "    else:\n",
    "      x = IdentityLayer(name='pre_logits')(x)\n",
    "    \n",
    "    if self.num_classes:\n",
    "      x = nn.Dense(\n",
    "        features=self.num_classes,\n",
    "        name='head',\n",
    "        kernel_init=nn.initializers.zeros)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vt5uGYJH3LXM"
   },
   "source": [
    "## Classification / Feature Extractor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "bjlhNR62-JJJ"
   },
   "outputs": [],
   "source": [
    "classification_model = True\n",
    "if classification_model:\n",
    "    num_classes = 1000\n",
    "else:\n",
    "    num_classes = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "mclDoMqbShzV"
   },
   "outputs": [],
   "source": [
    "config = common_config.get_config()\n",
    "config.model = models_config.AUGREG_CONFIGS[f\"{VIT_MODELS.split('-')[0]}\"]\n",
    "\n",
    "model = VisionTransformer(num_classes=num_classes, **config.model)\n",
    "\n",
    "path = f\"{ROOT_GCS_PATH}/{VIT_MODELS}.npz\"\n",
    "params = checkpoint.load(path)\n",
    "\n",
    "if not num_classes:\n",
    "    _ = params.pop(\"head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqbuBCFw9vyg"
   },
   "source": [
    "## Conversion\n",
    "\n",
    "Code has been reused from the official examples [here](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/examples/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XT2GLwXg95tE"
   },
   "source": [
    "### Step 1: Get a prediction function out of the JAX model & convert it to a native TF function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "J2e1h-F2SmDB"
   },
   "outputs": [],
   "source": [
    "predict_fn = lambda params, inputs: model.apply(\n",
    "    dict(params=params), inputs, train=False\n",
    ")\n",
    "\n",
    "with_gradient = False if num_classes else True\n",
    "tf_fn = jax2tf.convert(predict_fn, with_gradient=with_gradient, enable_xla=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKE1msyx-3ge"
   },
   "source": [
    "### Step 2: Set the trainability of the individual param groups and construct TF graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "8RNlRp9pTHgF"
   },
   "outputs": [],
   "source": [
    "trainable = False if num_classes else True\n",
    "param_vars = tf.nest.map_structure(\n",
    "    lambda param: tf.Variable(param, trainable=trainable), params\n",
    ")\n",
    "tf_graph = tf.function(\n",
    "    lambda inputs: tf_fn(param_vars, inputs), autograph=False, jit_compile=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fDRubHD_Sf3"
   },
   "source": [
    "### Step 3: Serialize as a SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "form",
    "id": "1QJQwDEyTs2V"
   },
   "outputs": [],
   "source": [
    "#@title SavedModel wrapper class utility from [here](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/examples/saved_model_lib.py#L128)\n",
    "class _ReusableSavedModelWrapper(tf.train.Checkpoint):\n",
    "  \"\"\"Wraps a function and its parameters for saving to a SavedModel.\n",
    "  Implements the interface described at\n",
    "  https://www.tensorflow.org/hub/reusable_saved_models.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, tf_graph, param_vars):\n",
    "    \"\"\"Args:\n",
    "      tf_graph: a tf.function taking one argument (the inputs), which can be\n",
    "         be tuples/lists/dictionaries of np.ndarray or tensors. The function\n",
    "         may have references to the tf.Variables in `param_vars`.\n",
    "      param_vars: the parameters, as tuples/lists/dictionaries of tf.Variable,\n",
    "         to be saved as the variables of the SavedModel.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    # Implement the interface from https://www.tensorflow.org/hub/reusable_saved_models\n",
    "    self.variables = tf.nest.flatten(param_vars)\n",
    "    self.trainable_variables = [v for v in self.variables if v.trainable]\n",
    "    # If you intend to prescribe regularization terms for users of the model,\n",
    "    # add them as @tf.functions with no inputs to this list. Else drop this.\n",
    "    self.regularization_losses = []\n",
    "    self.__call__ = tf_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xr2Vf9Ql_lca",
    "outputId": "2e4150aa-fdb8-4060-cb2b-8a0fbae7ee08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to R50_L_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224 directory.\n"
     ]
    }
   ],
   "source": [
    "input_signatures = [tf.TensorSpec(shape=[1, 224, 224, 3], dtype=tf.float32)]\n",
    "model_dir = VIT_MODELS if num_classes else f\"{VIT_MODELS}_fe\"\n",
    "signatures = {}\n",
    "saved_model_options = None\n",
    "\n",
    "print(f\"Saving model to {model_dir} directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMn9fJxuTKON",
    "outputId": "ec64c63d-e3ca-4eec-fc9e-87b73dd42861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: R50_L_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: R50_L_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224/assets\n"
     ]
    }
   ],
   "source": [
    "signatures[\n",
    "    tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n",
    "] = tf_graph.get_concrete_function(input_signatures[0])\n",
    "\n",
    "wrapper = _ReusableSavedModelWrapper(tf_graph, param_vars)\n",
    "if with_gradient:\n",
    "    if not saved_model_options:\n",
    "        saved_model_options = tf.saved_model.SaveOptions(\n",
    "            experimental_custom_gradients=True\n",
    "        )\n",
    "    else:\n",
    "        saved_model_options.experimental_custom_gradients = True\n",
    "tf.saved_model.save(\n",
    "    wrapper, model_dir, signatures=signatures, options=saved_model_options\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PJr-uVs_vz-"
   },
   "source": [
    "## Functional test (credits: [Willi Gierke](https://ch.linkedin.com/in/willi-gierke))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "XyvjkBAE5iFL"
   },
   "outputs": [],
   "source": [
    "#@title Image preprocessing utilities \n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = np.array(image)\n",
    "    image_resized = tf.image.resize(image, (224, 224))\n",
    "    image_resized = tf.cast(image_resized, tf.float32)\n",
    "    image_resized = (image_resized - 127.5) / 127.5\n",
    "    return tf.expand_dims(image_resized, 0).numpy()\n",
    "\n",
    "def load_image_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    image = preprocess_image(image)\n",
    "    return image\n",
    "\n",
    "!wget https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hd-YH-hqAIQ9"
   },
   "source": [
    "### Load image and ImageNet-1k class mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "4vDQd6MEAEp_"
   },
   "outputs": [],
   "source": [
    "with open(\"ilsvrc2012_wordnet_lemmas.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "imagenet_int_to_str = [line.rstrip() for line in lines]\n",
    "\n",
    "img_url = \"https://p0.pikrepo.com/preview/853/907/close-up-photo-of-gray-elephant.jpg\"\n",
    "image = load_image_from_url(img_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A-LxOYBANnv"
   },
   "source": [
    "### Inference\n",
    "\n",
    "This is only application for the classification models. For fine-tuning/feature extraction, please follow [this notebook] (TODO: Add link) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99DTGYB25o5d"
   },
   "outputs": [],
   "source": [
    "# Load the converted SavedModel and check whether it finds the elephant.\n",
    "restored_model = tf.saved_model.load(model_dir)\n",
    "predictions = restored_model.signatures[\"serving_default\"](tf.constant(image))\n",
    "logits = predictions[\"output_0\"][0]\n",
    "predicted_label = imagenet_int_to_str[int(np.argmax(logits))]\n",
    "expected_label = \"Indian_elephant, Elephas_maximus\"\n",
    "assert (\n",
    "    predicted_label == expected_label\n",
    "), f\"Expected {expected_label} but was {predicted_label}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itfezjKjAXA6"
   },
   "source": [
    "## Inference with TensorFlow Hub \n",
    "\n",
    "Run the following code snippet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HR8lV2377Ad3"
   },
   "source": [
    "```python\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "classification_model = tf.keras.Sequential([hub.KerasLayer(model_dir)])\n",
    "predictions = classification_model.predict(image)\n",
    "predicted_label = imagenet_int_to_str[int(np.argmax(logits))]\n",
    "predicted_label\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Owb6QWi6BaMr"
   },
   "source": [
    "## Chore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-_lGpHTs58RO"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth \n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ViQIXEnH7Lq4",
    "outputId": "de0378f9-1ed9-43ca-8454-a8cb0eef84d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://L_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224/saved_model.pb [Content-Type=application/octet-stream]...\n",
      "/ [0/3 files][    0.0 B/  1.1 GiB]   0% Done                                    \r",
      "Copying file://L_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224/variables/variables.index [Content-Type=application/octet-stream]...\n",
      "/ [0/3 files][    0.0 B/  1.1 GiB]   0% Done                                    \r",
      "Copying file://L_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224/variables/variables.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
      "/ [0/3 files][    0.0 B/  1.1 GiB]   0% Done                                    \r",
      "==> NOTE: You are uploading one or more large file(s), which would run\n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "|\n",
      "Operation completed over 3 objects/1.1 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -r {model_dir} gs://flowers-experimental"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "conversion",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
